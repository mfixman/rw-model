\documentclass[11pt,a4paper]{article}

\usepackage{microtype}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{xfrac}

\newcommand{\alphamack}{\alpha _{\text{Mack}}}
\newcommand{\alphahall}{\alpha _{\text{Hall}}}

\renewcommand{\thesubsection}{\arabic{subsection}}
\renewcommand{\thesubsubsection}{\quad{} \alph{subsubsection}}

\begin{document}

\section*{Canonical Formulas}

\clearpage{}

\section*{Changes to the Rescorla-Wagner Modified Model}
This document present a summary of the ideas we are using in our implementation of the Adjusted Rescola Wagner model.

\subsection{The problem}
The standard Rescola Wagner's model formulation do not consider a dynamic learning rate for the stimuli ($\alpha$); therefore, the model is not able to explain phenomena like latent inhibition or the Hall and Pearce effect.

Here, we present an Adjusted Rescola Wagner model that includes 2 different $\alpha$ factors which are able to account for phenomena that "requires" the attention \textit{à la Mackhintosh} as well as phenomena that are explainable by a new formulation of $\alpha$.

\subsection{Explanation}
Here we present the ideas and the explanation behind our Adjusted Rescola Wagner model. 

First of all, the foundation of our proposals is that the learning should be modulated by two different $\alpha$ factors:

\subsubsection{$\alphamack$}
This term reflects the attention \textit{à la Mackhintosh}.

\subsubsection{$\alphahall$}
This term reflects the "adaptability" factor in learning and should be able to account for phenomena like latent inhibition and Hall and Pearce effect. The idea is that when a subject experience the same pair of stimuli repeatedly, the learning of the association between them is reduced because the subject "adapts" to the stimuli. In principle, the dominance of this effect should scale up with the weakness of the US.  

A possible formulation of this term can be:
\begin{gather}
	\begin{aligned}
		\alphahall ^n &= - \alpha ^n \cdot \delta \cdot e^{- \frac{1}{2} \cdot 
		\displaystyle{ \left( \nabla_1 [ f ] ( n ) \right) ^2 }} \\
		&= - \alpha ^n \cdot \delta \cdot e ^ {- \frac{1}{2} \cdot \displaystyle{\left(V^n_{MA} - V^{n - 1}_{MA} \right) ^2}}
	\end{aligned}
\end{gather}
where $ \delta \in ( 0, 1 ) $ is an parameter of the model and $V^n_{MA}$ is the moving average of the associative strength $V$, defined by:
\begin{align}
    V ^n _{\text{MA}} (k) &= \frac{1}{k} \sum ^n _{i = n - k + 1} V ^i
\end{align}
where $k$ is the sliding window.

The moving average of $V$ reflects the fact that the subject not only update the associative strength $V_S$ of a certain stimulus $S$ with an outcome $O$, but also maintain an "internal memory state". That is, another lower level, yet parallel, degree of belief on how much the stimulus $S$ is able to predict the outcome $O$. This "internal memory state" takes into account not only the present but also the past experiences of the subject\footnote{The simple moving average might not be able to reflect the fact that experiences that are closer in time to the present are more relevant than the ones in the past. Therefore, it may be better to consider an exponential moving average or simply multiply $v^i$ by $\sfrac{i}{n}$ (discount factor).}. In this context, the sliding window reflect the "memory" of the subject; that is, a bigger sliding windows means that the subject is able to retrieve and use the information from a farther past.

\begin{figure}[h!]
    \centering
    \includegraphics[draft,width=0.8\textwidth]{alpha_Hall_function_plot.png}
    \caption{$\alphahall$ function plot with $\alpha^n = 1$ and $\delta = 1$}
    \label{fig:alpha_hall_plot}
\end{figure}

If we look at the graph in Figure \ref{fig:alpha_hall_plot}, we can notice that when $V^n_{MA}-V^{n-1}_{MA}$ is very small, $\alphahall^n$ tends to its maximum (negative) value ($\alpha^n \cdot \delta$). That is, when the subject is not updating its beliefs the associability between the present stimuli and the outcome is reduced. On the other hand, when the associative strength between a stimulus and a certain outcome increases rapidly, $V^n_{MA}-V^{n-1}_{MA}$ grows above $0$ and $\alphahall^n$ tends to zero. Similarly, when the associative strength decreases $V^n_{MA}-V^{n-1}_{MA}$ grows below $0$ and $\alphahall^n$ tends to zero\footnote{Someone may argue that the domain of the $\alphahall$ function should be $[-1,1]$ because the associative strength is clipped between $0$ and $1$; therefore, we should probably decrease the "variance" of the function to make sure that its value is close to zero at the extremes of the domain.}.

\subsection{Proposals}

We hereby present 3 alternatives of combining the two $\alpha$ factors that we presented.

\subsubsection{Double $\alpha$}
\begin{equation}
	\begin{aligned}
		\alpha ^{n + 1}
			&= \alpha ^n + \Delta \alpha ^n \\
			&= \alpha ^n + \left[ \lambda \alphamack ^n + (1 - \lambda) \alphahall ^n \right] 
	\end{aligned}
\end{equation}

The idea here is that, the strength of the US modules the two effects. That is, when the US is very strong (close to 1) the Mackintosh effect is predominant; while, when the US is weak (close to 0) the "Hall effect" dominates. Although this equation clarify how the two effect interact, we cannot use $\lambda$ to update $\alpha$; therefore, here's the formulation of the learning process:
\begin{equation}
    \begin{aligned}
    \Delta V^{n+1} &= \big( \alpha ^n + \left[ \lambda \alphamack ^n + (1 - \lambda) \alphahall ^n \right] \big) \cdot \frac{\Delta V^{n+1}_{R-W}}{\alpha^n_{R-W}}
    \end{aligned}
\end{equation}

\subsubsection{Maximum of both}

\begin{equation}
	\alpha ^{n + 1} = \max { \left( \alphamack ^n, \alphahall ^n \right) } 
\end{equation}

\subsubsection{Thresholding}

\begin{equation}
	\alpha ^{n + 1} = \begin{cases}
		\alphamack ^n & \text{if } V^n > \tau \\
		\alphahall ^n & \text{otherwise}
	\end{cases}
\end{equation}

\end{document}
